<!DOCTYPE html>
<html lang="es">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>IA ILEGALES</title>
    <link rel="stylesheet" href="styless.css">
    <link rel="icon" href="./img/ojo-ai-circular.png" type="image/png">
    <script src="script.js" defer></script>
</head>

<body>
    <header id="main-header">
        <div id="header-content">
            <a href="index.html">
                <img src="./img/Logo_IA.png" alt="Logo de Advertencia contra IA ILEGALES" id="logo">
            </a>
            <nav id="main-nav">
                <a href="index.html">
                    <span class="span-inicio">Inicio</span>
                </a>
                <a href="peligroIa.html">IA</a>
                <a href="blog.html">Blog</a>
                <a href="contact.html">Contacto</a>
            </nav>
        </div>
    </header>

    <main id="main-section">
        <div id="main-superior">
            <h1 id="titulo-principal">PELIGRO IA ILEGALES</h1>
            <p id="subtitulo-principal">Descubre los sistemas de inteligencia artificial más peligrosos y las
                implicaciones legales que conllevan.</p>
            <img src="./img/ia-maligna-horizontal.png" alt="Imagen de IA Peligrosa" id="img-ia-peligrosa">
        </div>
        <div id="content-main">
            <div id="menu-utilidades">
                <p>
                    MENU DE UTILIDADES:
                </p>
                <button id="btn-modo-oscuro">Modo Oscuro</button>
                <button id="btn-modo-claro">Modo Claro</button>
            </div>
            <div id="main-content">
                <section id="sistemas-ia-peligrosos">
                    <h2>Los Sistemas de IA Más Peligrosos del Mundo: Una Guía sobre Riesgos, Ilegalidad y el Futuro de
                        la
                        Regulación</h2>
                    <p>La inteligencia artificial está transformando nuestro mundo a un ritmo vertiginoso, ofreciendo
                        beneficios
                        sin precedentes. Sin embargo, su rápido avance ha creado una zona gris legal donde los sistemas
                        de
                        IA
                        más dañinos operan sin control. Este artículo explora la delgada línea entre lo "ilegal" y lo
                        "alegal",
                        y desvela las cuatro categorías de IA más peligrosas que representan una amenaza para la
                        seguridad,
                        la
                        democracia y los derechos humanos a nivel global.
                    </p>
                </section>
                <section id="riesgo-inaceptable">
                    <h2>1. El Riesgo Inaceptable: Más Allá de la "IA Ilegal"</h2>
                    <p>La pregunta sobre cuáles son las IA "ilegales y más peligrosas" revela una complejidad
                        fundamental:
                        la
                        tecnología avanza más rápido que la ley. Durante años, el uso de IA para actividades dañinas,
                        como
                        la
                        creación de contenido sintético falso, ha operado en un vacío jurídico, conocido como
                        "alegalidad".
                        Las
                        conductas que no están reguladas ni prohibidas por la ley no pueden ser penalizadas.  

                        El mayor peligro no radica en lo que ya está prohibido, sino en aquellos sistemas que operan en
                        este
                        espacio alegal. Una vez que los daños de estas aplicaciones se vuelven notorios (como la
                        discriminación
                        en el empleo o los fraudes masivos), se genera una presión pública para su regulación. La Ley de
                        Inteligencia Artificial (AI Act) de la Unión Europea ha respondido a este vacío, trasladando las
                        prácticas dañinas a categorías de "riesgo inaceptable" (prácticas prohibidas) o de "alto riesgo"
                        (sujetas a estricta regulación).  

                        El AI Act de la UE prohíbe explícitamente ocho prácticas de IA por ser una amenaza clara a los
                        derechos
                        fundamentales :  

                    </p>
                    <ul id="lista-peligros">
                        <li>Manipulación subliminal y engañosa.</li>
                        <li>Explotación de vulnerabilidades de personas o colectivos para modificar su comportamiento,
                            especialmente
                            si esto causa un perjuicio.</li>
                        <li>Puntuación social (social scoring) que clasifica a las personas en función de su
                            comportamiento
                            social
                            o
                            estatus, si esto genera un trato perjudicial o desfavorable.</li>
                        <li>Identificación biométrica remota en tiempo real en espacios públicos para fines policiales
                            (con
                            excepciones muy estrictas).</li>
                        <li>El raspado no selectivo de bases de datos de reconocimiento facial a partir de internet o de
                            sistemas
                            de
                            videovigilancia.</li>
                        <li>El uso de sistemas de IA para el análisis o predicción del comportamiento, características
                            personales o estados emocionales con fines publicitarios, de contratación laboral o toma de
                            decisiones automatizadas que afecten significativamente a las personas.</li>
                        <li>El uso de sistemas de IA que generen contenido sintético falso (deepfakes) sin una
                            divulgación
                            clara y adecuada.</li>
                        <li>El uso de sistemas de IA para el rastreo no selectivo y masivo de datos personales en línea
                            o en
                            redes sociales con fines publicitarios o de otro tipo.</li>
                    </ul>
                </section>
                <section id="usos-maliciosos">
                    <h2>2. Los Usos Maliciosos en la Práctica: Casos de Fraude y Manipulación Social</h2>
                    <p>La IA se ha convertido en una herramienta poderosa para la ciberdelincuencia y la desinformación.
                        Los
                        ciberdelincuentes utilizan la tecnología para crear campañas de  

                        phishing hiper-personalizadas y para manipular a las víctimas a través de deepfakes y la
                        clonación
                        de
                        voz.  

                        A continuación, se presentan algunos ejemplos documentados de 2024 que demuestran la
                        peligrosidad de
                        la
                        IA:
                    </p>
                    <table id="tabla-ejemplos">
                        <tr>
                            <th>Caso de fraude</th>
                            <th>Tecnología Utilizada</th>
                            <th>Impacto y Consecuencias</th>
                        </tr>
                        <tr>
                            <td>Campaña de phishing dirigida a empleados de una empresa financiera</td>
                            <td>Generación de correos electrónicos personalizados con IA</td>
                            <td>Robo de credenciales y acceso no autorizado a sistemas internos</td>
                        </tr>
                        <tr>
                            <td>Fraude a la empresa de ingeniería Arup</td>
                            <td>Deepfake en tiempo real (video y voz)</td>
                            <td>Un empleado fue engañado para autorizar 15 transacciones, lo que resultó en un fraude
                                financiero
                                de 25 millones de dólares. El empleado fue persuadido después de participar en una
                                videollamada
                                con recreaciones deepfake de sus ejecutivos.  

                            </td>
                        </tr>
                    </table>
                    <p>La IA también ha democratizado la creación de contenido falso, permitiendo a un gran número de
                        personas
                        generar imágenes y textos convincentes a bajo costo. 1 Esta desinformación a gran escala amenaza
                        la
                        capacidad de los ciudadanos para tomar decisiones informadas y socava la confianza en las
                        instituciones
                        democráticas. 2  
                        La manipulación de la opinión pública a través de bots y cuentas falsas en redes sociales es
                        otra
                        preocupación creciente. Estos sistemas pueden amplificar mensajes polarizadores y desestabilizar
                        procesos electorales.
                    </p>
                </section>
                <section id="amenaza-silenciosa">
                    <h2>3. La Amenaza Silenciosa: Sesgo y Discriminación Sistémica</h2>
                    <p>
                        El sesgo algorítmico es una amenaza menos visible pero igualmente peligrosa. El sesgo no reside
                        en
                        el algoritmo en sí, sino en los datos con los que se entrena el sistema y en el diseño humano.
                        Las
                        causas principales son:  
                    </p>
                    <ol>
                        <li>
                            <b>Sesgos en los datos de entrenamiento:</b> Si los datos son no representativos,
                            históricamente
                            sesgados o
                            incompletos, el algoritmo amplificará estos prejuicios. Un algoritmo de reconocimiento
                            facial
                            entrenado principalmente con rostros de personas blancas puede cometer más errores al
                            identificar a
                            personas de color.  
                        </li>
                        <li>
                            <b>Sesgos en el diseño algorítmico:</b> Errores de programación o la asignación de
                            ponderaciones
                            subjetivas
                            por parte de los desarrolladores pueden transferir sus propios sesgos al sistema.  
                        </li>
                        <li>
                            <b>Sesgos en los datos proxy:</b> Los sistemas pueden utilizar datos que no están
                            directamente
                            relacionados
                            con la raza o el género, pero que tienen una correlación inadvertida, lo que puede
                            perjudicar a
                            ciertos grupos.  
                        </li>
                        <h3>Estos sesgos han tenido consecuencias en el mundo real:</h3>

                        <li>
                            <b>Sistemas de justicia penal:</b> En 2021, un hombre afroamericano en Michigan fue
                            arrestado
                            injustamente
                            tras ser identificado erróneamente por un sistema de reconocimiento facial que había sido
                            entrenado
                            mayoritariamente con rostros blancos. La Unión Europea ha advertido que las tecnologías de
                            identificación basadas en algoritmos cometen más errores al identificar a personas
                            racializadas.
                             
                        </li>
                        <li>
                            <b>Empleo y servicios financieros:</b> Un algoritmo de contratación de Amazon fue descartado
                            tras
                            encontrar
                            que favorecía a los candidatos hombres. De manera similar, una investigación reveló que el
                            sistema
                            de publicidad de Google mostraba anuncios de puestos mejor pagados con más frecuencia a los
                            hombres.
                        </li>
                    </ol>
                    <section id="cuestion-critica">
                        <h2>4. La Cuestión Más Crítica: Armas Letales Autónomas (LAWS)</h2>
                        <p>
                            El desarrollo de las Armas Letales Autónomas (LAWS) representa la amenaza más crítica y
                            potencialmente existencial de la IA. Las LAWS son sistemas militares (drones o robots) que
                            pueden buscar y atacar objetivos por sí mismos sin una intervención humana posterior.  

                            Amnistía Internacional, la ONU y la Cruz Roja Internacional han instado a la prohibición de
                            estos sistemas, argumentando que las máquinas son incapaces de valorar la vida humana o
                            ejercer un juicio moral. El debate se centra en la necesidad de mantener un "control humano
                            significativo" sobre todas las decisiones de vida o muerte. La falta de un consenso global y
                            de un tratado vinculante ha llevado a un estancamiento en la regulación, lo que permite que
                            esta tecnología avance en un estado de alegalidad global. Este "riesgo existencial alegal"
                            podría conducir a una carrera armamentista irreversible.  
                        </p>
                    </section>
                    <section id="conclusion">
                        <h2>Conclusión: El Desafío de la Gobernanza Global</h2>
                        <p>
                            Las IA "ilegales" más peligrosas no son necesariamente las que están explícitamente
                            prohibidas, sino aquellas que operan en los vacíos regulatorios, amplifican sesgos y
                            manipulan a la sociedad. La fragmentación regulatoria a nivel mundial agrava el problema,
                            permitiendo que las tecnologías abusivas sean exportadas a jurisdicciones con leyes más
                            laxas.  

                            Para mitigar estos riesgos, es crucial pasar de un enfoque regional a una gobernanza global
                            unificada. Se necesitan estándares internacionales vinculantes y un compromiso con la
                            transparencia y la supervisión humana en todos los sistemas de alto riesgo. La prohibición
                            de las armas letales autónomas es el paso más urgente para asegurar que el desarrollo de la
                            IA se alinee con los derechos humanos y la seguridad global.

                            La regulación efectiva de la IA requiere una colaboración estrecha entre gobiernos,
                            industria, sociedad civil y expertos técnicos. Solo a través de un esfuerzo concertado
                            podemos
                            garantizar que la inteligencia artificial se utilice para el beneficio de toda la humanidad,
                            minimizando sus riesgos y evitando que las tecnologías más peligrosas operen en la sombra de
                            la alegalidad.
                        </p>
                    </section>
                </section>
            </div>
        </div>
    </main>

    <footer>
        <p>&copy; 2023 Mi Página Web</p>
    </footer>
</body>

</html>